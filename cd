#Step 1 Import all the python modules needed for the codes

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


#Step 2 Read Input data provided in json lien delimeted format
#DF=pd.read_json(path_or_buf ="transactions.txt", lines=True)

#Question 1 - 

display("1.2 Shape of the input dataset",DF.shape)

display("1.3 Summary Statistics Numeric",DF.describe(include='number',percentiles=[0,.05,.1,.2,.25,.5,.75,.80,.9,.95,.99]))
display("1.3 Summary Statistics Non Numeric",DF.describe(exclude='number').T)


BlankDF=pd.DataFrame(columns=['Field','Blanks'])
#blnk=pd.DataFrame(columns=['Field','Blanks'])
for var in DF.columns:
    if DF[DF[var] == ''].shape[0]>0:
        #blnk['Field']=var
        #blnk['Blanks']=DF[DF[var] == ''].shape[0]
        #BlankDF = BlankDF.append(blnk,ignore_index=True)
        BlankDF = BlankDF.append({'Field' : var , 'Blanks' : DF[DF[var] == ''].shape[0]} , ignore_index=True)
        #display(var,DF[DF[var] == ''].shape[0])
        
display("1.4 Number of Blank records for fields with blanks",BlankDF)
display("1.4 Number of Missing records",DF.isna().sum())

display("1.4 No. of Unique records",DF.nunique().sort_values(ascending=False))


plt.rcParams["figure.figsize"] = (20,5)

fig, ax = plt.subplots()
ax2 = ax.twinx()
n, bins, patches = ax.hist(DF['transactionAmount'], bins=100, density=True)
n, bins, patches = ax2.hist(DF['transactionAmount'], cumulative=1, histtype='step', bins=100, color='tab:orange')
ax.set_xlim((ax.get_xlim()[0], DF['transactionAmount'].max()))

plt.title('All')
plt.show()


plt.rcParams["figure.figsize"] = (25,10)
df = DF[['transactionAmount','merchantCategoryCode']]
#df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A','B', 'B', 'B', 'B', 'B'])
boxplot = df.boxplot(by='merchantCategoryCode', vert=False)


display(DF.groupby('merchantCategoryCode')['transactionAmount'].describe().sort_values(by='mean',ascending=False))



plt.rcParams["figure.figsize"] = (20,5)
fig, ax = plt.subplots()
ax2 = ax.twinx()
n, bins, patches = ax.hist(DF['transactionAmount'][DF['isFraud']==False], bins=100, density=True)
n, bins, patches = ax2.hist(DF['transactionAmount'][DF['isFraud']==False], cumulative=1, histtype='step', bins=100, color='tab:orange')
ax.set_xlim((ax.get_xlim()[0], DF['transactionAmount'][DF['isFraud']==False].max()))
plt.title('Non Fraud')
plt.show()

fig, ax = plt.subplots()
ax2 = ax.twinx()
n, bins, patches = ax.hist(DF['transactionAmount'][DF['isFraud']==True], bins=100, density=True)
n, bins, patches = ax2.hist(DF['transactionAmount'][DF['isFraud']==True], cumulative=1, histtype='step', bins=100, color='tab:orange')
ax.set_xlim((ax.get_xlim()[0], DF['transactionAmount'][DF['isFraud']==True].max()))
plt.title('Fraud')
plt.show()


DF['log_transactionAmount']=np.log(np.maximum(DF['transactionAmount'],.0001))
plt.rcParams["figure.figsize"] = (20,5)
fig, ax = plt.subplots()
ax2 = ax.twinx()
n, bins, patches = ax.hist(DF['log_transactionAmount'][DF['isFraud']==False], bins=100, density=True)
n, bins, patches = ax2.hist(DF['log_transactionAmount'][DF['isFraud']==False], cumulative=1, histtype='step', bins=100, color='tab:orange')
ax.set_xlim((ax.get_xlim()[0], DF['log_transactionAmount'][DF['isFraud']==False].max()))

plt.show()

fig, ax = plt.subplots()
ax2 = ax.twinx()
n, bins, patches = ax.hist(DF['log_transactionAmount'][DF['isFraud']==True], bins=100, density=True)
n, bins, patches = ax2.hist(DF['log_transactionAmount'][DF['isFraud']==True], cumulative=1, histtype='step', bins=100, color='tab:orange')
ax.set_xlim((ax.get_xlim()[0], DF['log_transactionAmount'][DF['isFraud']==True].max()))
plt.show()

#Step 1 Import all the python modules needed for the codes

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


#Step 2 Read Input data provided in json lien delimeted format
#DF=pd.read_json(path_or_buf ="transactions.txt", lines=True)

#Question 1 - 

#display(DF.shape)
#display(CountNa=DF.isna().sum())
#display(DF.describe(include='all'))


#Step 3 Descriptive Statistics of original columns in input DataFrame

#Object_describe=DF.describe(include='object')
#Int_describe=DF.describe(include='int')
#Float_describe=DF.describe(include='float')
#Bool_describe=DF.describe(include='bool')

#Univariate_describe=DF.describe(include='all')
#CountNa=DF.isna().sum()

#UniqueDF=DF.nunique().sort_values()


DF['isFraud_']=DF['isFraud']*1
DF['wgt']=1
DF['notFraud']=DF['wgt']-DF['isFraud_']
DF['fraud_transactionAmount'] = DF['transactionAmount']*DF['isFraud_']
DF['notfraud_transactionAmount'] = DF['transactionAmount']*DF['notFraud']
DF['REVERSAL']=(DF['transactionType']=='REVERSAL')*1


DF['log_transactionAmount']=np.log(np.maximum(DF['transactionAmount'],.0001))

DF['accountOpenDate_new'] = pd.to_datetime(DF['accountOpenDate'],format='%Y-%m-%d %H:%M:%S',errors='coerce')
DF['dateOfLastAddressChange_new'] = pd.to_datetime(DF['dateOfLastAddressChange'],format='%Y-%m-%d %H:%M:%S',errors='coerce')
DF['transactionDateTime_new'] = pd.to_datetime(DF['transactionDateTime'],format='%Y-%m-%d',errors='coerce')
DF['currentExpDate_new'] = pd.to_datetime(DF['currentExpDate'])

DF['transactionDateTime_year']=DF['transactionDateTime_new'].dt.year
DF['transactionDateTime_month']=DF['transactionDateTime_new'].dt.month
DF['transactionDateTime_day']=DF['transactionDateTime_new'].dt.day
DF['transactionDateTime_hour']=DF['transactionDateTime_new'].dt.hour
DF['transactionDateTime_minute']=DF['transactionDateTime_new'].dt.minute
DF['transactionDateTime_seconds']=DF['transactionDateTime_new'].dt.second
DF['transactionDateTime_dayofweek']=DF['transactionDateTime_new'].dt.weekday

DF['account_tenure']=DF['transactionDateTime_new']-DF['accountOpenDate_new']
DF['address_change_tenure']=DF['transactionDateTime_new']-DF['dateOfLastAddressChange_new']
DF['dateOfLastAddressChange_accountOpenDate']=DF['dateOfLastAddressChange_new']-DF['accountOpenDate_new']
DF['transaction_currentExpDate']=DF['currentExpDate_new']-DF['transactionDateTime_new']
DF['dateOfLastAddressChange_currentExpDate']=DF['currentExpDate_new']-DF['dateOfLastAddressChange_new']
DF['accountOpenDate_currentExpDate']=DF['currentExpDate_new']-DF['accountOpenDate_new']

DF['account_tenure']=DF['account_tenure'].dt.days
DF['address_change_tenure']=DF['address_change_tenure'].dt.days
DF['dateOfLastAddressChange_accountOpenDate']=DF['dateOfLastAddressChange_accountOpenDate'].dt.days
DF['transaction_currentExpDate']=DF['transaction_currentExpDate'].dt.days
DF['dateOfLastAddressChange_currentExpDate']=DF['dateOfLastAddressChange_currentExpDate'].dt.days
DF['accountOpenDate_currentExpDate']=DF['accountOpenDate_currentExpDate'].dt.days

DF["mapped_merchant"] = DF["merchantName"].map(merchant_mapping)
DF["mapped_merchant_catCode"]=DF["mapped_merchant"]+' '+DF["merchantCategoryCode"]


for vars in DF.select_dtypes(include='number').columns:
    DF[vars+'_bin']=pd.qcut(DF[vars],20, labels=False, retbins=False, duplicates='drop')
    
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

plt.rcParams["figure.figsize"] = (20,3)
DF_pivotmaster = pd.DataFrame({},index=[])


for vars in DF.columns:
    if vars not in ['isFraud_','notFraud','wgt','isFraud__bin','wgt_bin','notFraud_bin']:   

        #print(vars)
        if 1< DF[vars].nunique():
            DF_pivot=DF.pivot_table(index=vars,values=['wgt','isFraud_','notFraud'],aggfunc=np.sum)
            DF_pivot['FraudRate'] = DF_pivot['isFraud_'] / DF_pivot['wgt']
            DF_pivot['notFraud_share']=DF_pivot['notFraud']/DF_pivot['notFraud'].sum()
            DF_pivot['Fraud_share']=DF_pivot['isFraud_']/DF_pivot['isFraud_'].sum()
            DF_pivot['KS']=DF_pivot['isFraud_']/DF_pivot['isFraud_'].sum()-DF_pivot['notFraud']/DF_pivot['notFraud'].sum()
            DF_pivot['variable']=vars
            DF_pivot['values']=DF_pivot.index
            DF_pivot.reset_index(drop=True,inplace=True)
            DF_pivot.sort_values(by='FraudRate',ascending=False)
            DF_pivotmaster = DF_pivotmaster.append(DF_pivot,ignore_index=True)
        
        if 1< DF[vars].nunique() < 61:
            width = .35 # width of a bar

            DF_pivot[['notFraud_share','Fraud_share']].plot(kind='bar', width = width)
            DF_pivot['FraudRate'].plot(secondary_y=True)
            ax = plt.gca()
            plt.xlim([-width, len(DF_pivot['notFraud_share'])-width])
            ax.set_xticklabels((DF_pivot['values']))
            plt.title(vars)
            plt.ylabel("FraudRate")
            plt.show()
            
           DF['composite1']=DF.index
DF.sort_values(by=['accountNumber','transactionDateTime'],inplace=True)
DF.reset_index(inplace=True,drop=True)
DF['composite2']=DF.index
DF.sort_values(by=['merchantName','transactionDateTime'],inplace=True)
DF.reset_index(inplace=True,drop=True)
DF['composite3']=DF.index
DF.sort_values(by=['accountNumber','merchantName','transactionDateTime'],inplace=True)
DF.reset_index(inplace=True,drop=True)
DF['composite4']=DF.index
DF.sort_values(by=['merchantName','accountNumber','transactionDateTime'],inplace=True)
DF.reset_index(inplace=True,drop=True)
DF['composite5']=DF.index

DF.sort_values(by=['composite4'],inplace=True)
DF['first_cm_merch']=(DF['accountNumber']!=DF['accountNumber'].shift(+1))*1 | (DF['merchantName']!=DF['merchantName'].shift(+1))*1
DF['prior_transactions_lag'] = (DF['transactionDateTime_new'] -  DF['transactionDateTime_new'].shift())
DF['prior_transactions_lag']=DF['prior_transactions_lag'].dt.total_seconds()*(1-DF['first_cm_merch'])
DF['prior_transactions_lag_cm_merch']=DF['prior_transactions_lag']
DF['prior_transactions_lag'].fillna(pd.Timedelta(seconds=0))
DF['cumsum'] = DF['first_cm_merch'].cumsum()


DF['prior_transactions_lag_cm_merch']=DF['prior_transactions_lag']


DF['new_currentExpDate']=(DF['currentExpDate']!=DF['currentExpDate'].shift(+1))*1
DF['transactionAmount_lagdiff'] = (DF['transactionAmount'] -  DF['transactionAmount'].shift())
DF['transactionAmount_lagdiff']=DF['transactionAmount_lagdiff']*(1-DF['first_cm_merch'])+DF['transactionAmount']*(DF['first_cm_merch'])


DF['prior_total_transactions_cm_merch']= DF.groupby(['cumsum'])['wgt'].cumsum()-1
DF['prior_total_transaction_lag_cm_merch']= DF.groupby(['cumsum'])['prior_transactions_lag'].cumsum()
DF['prior_total_fraud_cm_merch']= DF.groupby(['cumsum'])['isFraud_'].cumsum()-DF['isFraud_']

DF['prior_total_transactionAmount_cm_merch']= DF.groupby(['cumsum'])['transactionAmount'].cumsum()-DF['transactionAmount']
DF['prior_total_fraud_transactionAmount_cm_merch']= DF.groupby(['cumsum'])['fraud_transactionAmount'].cumsum()-DF['fraud_transactionAmount']

DF['prior_fraud_rate_cm_merch']= DF['prior_total_fraud_cm_merch']/DF['prior_total_transactions_cm_merch']
DF['prior_fraudTA_rate_cm_merch']= DF['prior_total_fraud_transactionAmount_cm_merch']/DF['prior_total_fraud_transactionAmount_cm_merch']

DF['transactions_since_last_fraud_cm_merch']= DF.groupby(['cumsum','prior_total_fraud_cm_merch'])['wgt'].cumsum().div((DF['prior_total_fraud_cm_merch']>0))-1
DF['time_since_last_fraud_cm_merch']= DF.groupby(['cumsum','prior_total_fraud_cm_merch'])['prior_transactions_lag'].cumsum().div((DF['prior_total_fraud_cm_merch']>0))
DF['transactionAmount_since_last_fraud_cm_merch']= DF.groupby(['cumsum','prior_total_fraud_cm_merch'])['transactionAmount'].cumsum().div((DF['prior_total_fraud_cm_merch']>0))-DF['transactionAmount']

DF['prior_avg_transactions_lag_cm_merch']=DF['prior_total_transaction_lag_cm_merch']/DF['prior_total_transactions_cm_merch']
DF['prior_avg_transactionAmount_cm_merch']=DF['prior_total_transactionAmount_cm_merch']/DF['prior_total_transactions_cm_merch']

table=pd.crosstab(DF['merchantCategoryCode'],DF['REVERSAL'])
table=table.sort_values(by=1,ascending=True)
#table=table[-50:]
plt.rcParams["figure.figsize"] = (5,5)
#table.plot(kind='barh',stacked=True)
table.div(table.sum(1).astype(float),axis=0).plot(kind='barh',stacked=True)
plt.show()

table=pd.crosstab(DF['merchantCategoryCode'],DF['duplicated'])
table=table.sort_values(by=1,ascending=True)
#table=table[-50:]
plt.rcParams["figure.figsize"] = (5,5)
#table.plot(kind='barh',stacked=True)
table.div(table.sum(1).astype(float),axis=0).plot(kind='barh',stacked=True)
plt.show()

table=pd.crosstab(DF['mapped_merchant_catCode'],DF['isFraud_'])
table.sort_values(by=1,ascending=True,inplace=True)
table=table[-50:]
plt.rcParams["figure.figsize"] = (5,20)
table.div(table.sum(1).astype(float),axis=0).plot(kind='barh',stacked=True)
plt.show()

table=pd.crosstab(DF['merchantCategoryCode'],DF['isFraud_'])
table.sort_values(by=1,ascending=True,inplace=True)
plt.rcParams["figure.figsize"] = (10,10)
table.div(table.sum(1).astype(float),axis=0).plot(kind='barh',stacked=True)
plt.show()

table=pd.crosstab(DF['accountNumber'],DF['isFraud_'])
table=table.sort_values(by=1,ascending=True)
table=table[-50:]
plt.rcParams["figure.figsize"] = (5,20)
#table.plot(kind='barh',stacked=True)
table.div(table.sum(1).astype(float),axis=0).plot(kind='barh',stacked=True)
plt.show()

DF1.sort_values(by=['composite2'],inplace=True)
DF1['new_id']=(DF1['accountNumber']!=DF1['accountNumber'].shift(+1))*1

DF1['prior_transactions_lag'] = (DF1['transactionDateTime_new'] -  DF1['transactionDateTime_new'].shift())
DF1['prior_transactions_lag']=DF1['prior_transactions_lag'].dt.total_seconds()*(1-DF1['new_id'])
DF1['prior_transactions_lag'].fillna(pd.Timedelta(seconds=0))
DF1['cumsum'] = DF1['new_id'].cumsum()

DF1['prior_total_transactions_cm']= DF1.groupby(['cumsum'])['wgt'].cumsum()-1
DF1['prior_total_transaction_lag_cm']= DF1.groupby(['cumsum'])['prior_transactions_lag'].cumsum()
DF1['prior_total_fraud_cm']= DF1.groupby(['cumsum'])['isFraud_'].cumsum()-DF1['isFraud_']

DF1['prior_total_transactionAmount_cm']= DF1.groupby(['cumsum'])['transactionAmount'].cumsum()-DF1['transactionAmount']
DF1['prior_total_fraud_transactionAmount_cm']= DF1.groupby(['cumsum'])['fraud_transactionAmount'].cumsum()-DF1['fraud_transactionAmount']

DF1['prior_fraud_rate_cm']= DF1['prior_total_fraud_cm']/DF1['prior_total_transactions_cm']
DF1['prior_fraudTA_rate_cm']= DF1['prior_total_fraud_transactionAmount_cm']/DF1['prior_total_transactionAmount_cm']

DF1['transactions_since_last_fraud_cm']= DF1.groupby(['cumsum','prior_total_fraud_cm'])['wgt'].cumsum().div((DF1['prior_total_fraud_cm']>0))-1
DF1['time_since_last_fraud_cm']= DF1.groupby(['cumsum','prior_total_fraud_cm'])['prior_transactions_lag'].cumsum().div((DF1['prior_total_fraud_cm']>0))
DF1['transactionAmount_since_last_fraud_cm']= DF1.groupby(['cumsum','prior_total_fraud_cm'])['transactionAmount'].cumsum().div((DF1['prior_total_fraud_cm']>0))-DF1['transactionAmount']


DF1['prior_avg_transactions_lag_cm']=DF1['prior_total_transaction_lag_cm']/DF1['prior_total_transactions_cm']
DF1['prior_avg_transactionAmount_cm']=DF1['prior_total_transactionAmount_cm']/DF1['prior_total_transactions_cm']

DF1.sort_values(by=['composite3'],inplace=True)
DF1['new_id']=(DF1['merchantName']!=DF1['merchantName'].shift(+1))*1

DF1['prior_transactions_lag'] = (DF1['transactionDateTime_new'] -  DF1['transactionDateTime_new'].shift())
DF1['prior_transactions_lag']=DF1['prior_transactions_lag'].dt.total_seconds()*(1-DF1['new_id'])
DF1['prior_transactions_lag'].fillna(pd.Timedelta(seconds=0))
DF1['cumsum'] = DF1['new_id'].cumsum()

DF1['prior_total_transactions_merch']= DF1.groupby(['cumsum'])['wgt'].cumsum()-1
DF1['prior_total_transaction_lag_merch']= DF1.groupby(['cumsum'])['prior_transactions_lag'].cumsum()
DF1['prior_total_fraud_merch']= DF1.groupby(['cumsum'])['isFraud_'].cumsum()-DF1['isFraud_']

DF1['prior_total_transactionAmount_merch']= DF1.groupby(['cumsum'])['transactionAmount'].cumsum()-DF1['transactionAmount']
DF1['prior_total_fraud_transactionAmount_merch']= DF1.groupby(['cumsum'])['fraud_transactionAmount'].cumsum()-DF1['fraud_transactionAmount']

DF1['prior_fraud_rate_merch']= DF1['prior_total_fraud_merch']/DF1['prior_total_transactions_merch']
DF1['prior_fraudTA_rate_merch']= DF1['prior_total_fraud_transactionAmount_merch']/DF1['prior_total_transactionAmount_merch']

DF1['transactions_since_last_fraud_merch']= DF1.groupby(['cumsum','prior_total_fraud_merch'])['wgt'].cumsum().div((DF1['prior_total_fraud_merch']>0))-1
DF1['time_since_last_fraud_merch']= DF1.groupby(['cumsum','prior_total_fraud_merch'])['prior_transactions_lag'].cumsum().div((DF1['prior_total_fraud_merch']>0))
DF1['transactionAmount_since_last_fraud_merch']= DF1.groupby(['cumsum','prior_total_fraud_merch'])['transactionAmount'].cumsum().div((DF1['prior_total_fraud_merch']>0))-DF1['transactionAmount']

DF1['prior_avg_transactions_lag_merch']=DF1['prior_total_transaction_lag_merch']/DF1['prior_total_transactions_merch']
DF1['prior_avg_transactionAmount_merch']=DF1['prior_total_transactionAmount_merch']/DF1['prior_total_transactions_merch']

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

plt.rcParams["figure.figsize"] = (20,3)
DF_pivotmaster = pd.DataFrame({},index=[])

for vars in DF1.columns:
    if vars not in DF.columns:  
        DF1[vars+'_bin_no']=pd.qcut(DF1[vars],20, labels=False, retbins=False, duplicates='drop')
        
for vars in DF1.columns:
    if vars not in DF.columns:  
        #print(vars)
        if 1< DF1[vars].nunique():
            DF_pivot=DF1.pivot_table(index=vars,values=['wgt','isFraud_','notFraud'],aggfunc=np.sum)
            DF_pivot['FraudRate'] = DF_pivot['isFraud_'] / DF_pivot['wgt']
            DF_pivot['notFraud_share']=DF_pivot['notFraud']/DF_pivot['notFraud'].sum()
            DF_pivot['Fraud_share']=DF_pivot['isFraud_']/DF_pivot['isFraud_'].sum()
            DF_pivot['KS']=DF_pivot['isFraud_']/DF_pivot['isFraud_'].sum()-DF_pivot['notFraud']/DF_pivot['notFraud'].sum()
            DF_pivot['variable']=vars
            DF_pivot['values']=DF_pivot.index
            DF_pivot.reset_index(drop=True,inplace=True)
            DF_pivot.sort_values(by='FraudRate',ascending=False)
            DF_pivotmaster = DF_pivotmaster.append(DF_pivot,ignore_index=True)
        print(vars,DF1[vars].nunique())
        if 1< DF1[vars].nunique() < 61:
            width = .35 # width of a bar

            DF_pivot[['notFraud_share','Fraud_share']].plot(kind='bar', width = width)
            DF_pivot['FraudRate'].plot(secondary_y=True)
            ax = plt.gca()
            plt.xlim([-width, len(DF_pivot['notFraud_share'])-width])
            ax.set_xticklabels((DF_pivot['values']))
            plt.title(vars)
            plt.ylabel("FraudRate")
            plt.show()
            
            
            #predictors=['pred_depvartime_1','pred_depvar1time_0','pred_depvar3time_1','pred_depvar5time_1']
#outtimefeb['depvar'] = outtimefeb['wgt']
#gbm0 = GradientBoostingClassifier(learning_rate=0.1,random_state=10,n_estimators=100,verbose=1)
gbm0 = GradientBoostingClassifier(learning_rate=0.1,random_state=10,verbose=1,n_estimators=100,validation_fraction=0.2,min_samples_split=2000,n_iter_no_change=10)

print("block 8")
train_prob1,test_prob1,val_prob1,feat_imp=modelfit(gbm0,train70pctDF,test30pct,outtimeDF, predictors,printFeatureImportance=True,depvar='depvar')
print("block 9")
train_prob1['wgt']=1
test_prob1['wgt']=1
val_prob1['wgt']=1


#train_prob1.to_csv("/u1/neerkuma/ai_challenge/ads/submissions/train_prob.csv")
#test_prob1.to_csv("/u1/neerkuma/ai_challenge/ads/submissions_testdummies/test_prob.csv")
#val_prob1.to_csv("/u1/neerkuma/ai_challenge/ads/submissions/val_prob.csv")
feat_imp.to_csv('/u1/neerkuma/ai_challenge/ads/oct4/feat_imp.csv')

#test_prob1['Did_Fail']=test_prob1['predicted']
#test_prob1=test_feb_orig[['fund','invnum']].merge(test_prob1,how='left',on=['fund','invnum'])
#test_prob1.to_csv('/u1/neerkuma/ai_challenge/ads/oct4/PANK_pred_v27.csv',columns=['Did_Fail'],header='Did_Fail',index=False)


gini(train_prob1,'actual','predicted','/u1/neerkuma/ai_challenge/ads/oct4/train_lorenz','wgt')
gini(test_prob1,'actual','predicted','/u1/neerkuma/ai_challenge/ads/oct4/test_lorenz','wgt')
gini(val_prob1,'actual','predicted','/u1/neerkuma/ai_challenge/ads/oct4/val_lorenz','wgt')

depVar = 'depvar'
keyVar = 'key'
wgtVar = 'wgt'
def gini(inputDF,actualCol,scoreCol,lbl,wgtCol=None):
    if wgtCol is None:
        wgtCol=1
    binCount = 10
    binSize = np.ceil(inputDF.shape[0]/binCount)
    inputDF["wgtScore"] = inputDF[wgtVar]*inputDF[scoreCol]
    inputDF["wgtActual"] = inputDF[wgtVar]*inputDF[actualCol]
    
    inputDF = inputDF.sort_values(by = [actualCol],ascending = False).reset_index(drop=True)
    inputDF["actcumwgt"] = inputDF[wgtVar].cumsum()
    inputDF["actualBin"] = np.floor((inputDF.actcumwgt)/binSize)+1
    
    inputDF = inputDF.sort_values(by = [scoreCol],ascending = False).reset_index(drop=True)
    inputDF["scrcumwgt"] = inputDF[wgtVar].cumsum()
    inputDF["scoreBin"] = np.floor((inputDF.scrcumwgt)/binSize)+1
    
    mergedDF = inputDF[wgtCol].groupby(inputDF["actualBin"],as_index=True).sum().to_frame("count")
    
    mergedDF["bestMean"] = inputDF["wgtActual"].groupby(inputDF["actualBin"],as_index=True).sum()/mergedDF["count"]
    mergedDF["respMean"] = inputDF["wgtActual"].groupby(inputDF["scoreBin"],as_index=True).sum()/mergedDF["count"]
    mergedDF["predMean"] = inputDF["wgtScore"].groupby(inputDF["scoreBin"],as_index=True).sum()/mergedDF["count"]
    
    mergedDF["bestPct"] = mergedDF["bestMean"]/mergedDF["bestMean"].sum()
    mergedDF["respPct"] = mergedDF["respMean"]/mergedDF["respMean"].sum()  
    
    mergedDF["bestCumPct"] = mergedDF["bestPct"].cumsum()
    mergedDF["respCumPct"] = mergedDF["respPct"].cumsum()
    mergedDF["randomLine"] = mergedDF.index.get_values()/binCount
    
    mergedDF["SumAct"] = inputDF["wgtActual"].groupby(inputDF["scoreBin"],as_index=True).sum()
    mergedDF["SumScr"] = inputDF["wgtScore"].groupby(inputDF["scoreBin"],as_index=True).sum() 
    
    mergedDF.to_csv(lbl+".csv")
    
    respAUC = np.sum(mergedDF["respCumPct"] - mergedDF["randomLine"])/binCount
    bestAUC = np.sum(mergedDF["bestCumPct"] - mergedDF["randomLine"])/binCount
    gini = respAUC/bestAUC
    
    accuracy = 1 - np.abs(mergedDF["predMean"] - mergedDF["respMean"]).sum()/mergedDF["respMean"].sum()
    return ["{0:0.2f}%".format(gini*100),"{0:0.2f}%".format(accuracy*100),"{0:0.2f}%".format(mergedDF['respPct'].values[0]*100)]

def modelfit(alg,dtrain,dtest,dval,predictors, performCV=False, printFeatureImportance=True, cv_folds=2,depvar='depvar'):
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain[depvar])
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
    dtest_predprob = alg.predict_proba(dtest[predictors])[:,1]
    dval_predprob = alg.predict_proba(dval[predictors])[:,1]


    #Perform cross-validation:
    if performCV:
        cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain['depvar'], cv=cv_folds, scoring='roc_auc')
        #cv_score = cross_val_score(alg, dtrain[predictors], dtrain['depvar'], cv=cv_folds, scoring='roc_auc')
    
        #Print model report:
        print("\nModel Report")
        print("Accuracy : %.4g" % metrics.accuracy_score(dtrain['depvar'].values, dtrain_predictions))
        print("AUC Score (Train): %f" % metrics.roc_auc_score(dtrain['depvar'], dtrain_predprob))

    if performCV:
        print("CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))
        
    #Print Feature Importance:
    if printFeatureImportance:
        feat_imp = pd.Series(alg.feature_importances_, predictors)
        feat_imp1=pd.DataFrame(feat_imp)
        #feat_imp1.to_csv("feat_imp1.csv")
    #feat_imp.plot(kind='bar', title='Feature Importances')
    #plt.ylabel('Feature Importance Score'

    np.column_stack((dtest[depvar].values,dtest_predprob))
    return(pd.DataFrame(np.column_stack((dtrain[depvar].values,dtrain_predprob)),columns=['actual','predicted']),pd.DataFrame(np.column_stack((dtest[depvar].values,dtest_predprob)),columns=['actual','predicted']),pd.DataFrame(np.column_stack((dval[depvar].values,dval_predprob)),columns=['actual','predicted']),feat_imp1)
    
    #Step 1 : import all fucntioanlities
import pandas as pd
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

from sklearn.model_selection import train_test_split
#from sklearn.cross_validation import train_test_split
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm
#from sklearn import cross_validation, metrics   #Additional scklearn functions
#from sklearn.grid_search import GridSearchCV   #Perforing grid search
from sklearn.model_selection import learning_curve, GridSearchCV

import pandas.core.algorithms as algos
from pandas import Series
import scipy.stats.stats as stats
import re
import traceback
import string
